\documentclass[Location Location Location! : Exploring Image Segmentation Problem In Urban Driving Scenarios]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=green,
    filecolor=magenta,      
    urlcolor=green,
    pdftitle={},
    pdfpagemode=FullScreen,
    }



\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Exploring the Semantic Segmentation Problem on Urban Driving Datasets\\

}

\author{\IEEEauthorblockN{1\textsuperscript{st} Komel Merchant}
\IEEEauthorblockA{\textit{Electrical Engineering} \\
\textit{Johns Hopkins University}\\
Baltimore, Maryland \\
kmercha2@jh.edu}
}

\maketitle

\begin{abstract}
This paper explores one potential solution to the vehicle semantic segmentation problem in the context of urban autonomous driving. In this paper, I explore 2 issue within the space. (1) Is the problem is of dealing with unbalanced datasets. Specifically, I look at the DICE loss and compare performance to the conventional binary cross-entropy loss. (2) Is the question of network scalability with respect to location. I train a U-Net on semantic data from the KITTI \cite{kitti} dataset. I then evaluate the models' qualitative performance on a custom DC street dataset acquired from a standard iPhone camera. For source code on this project, see \url{https://github.com/krmerchant/jhu_midterm/tree/main}


\end{abstract}

\begin{IEEEkeywords}
semantic segmentation, deep learning, u-net, urban driving, robotics
\end{IEEEkeywords}



\section{Introduction}  
The capability to detect and semantically segment cars in an urban scene is an important problem in many different domains. Being able to track vehicle positions over time is crucial task within robotic perception, traffic estimation and many other technical innovations in urban situations. As noted in reference \cite{sensor-fusion}, estimating the state of objects in an urban scene is a core task for any online perception module operating on an autonomous vehicle. Robust autonomous systems make use of several different modalities and fuse detections across these modalities to produce consistent state estimates about the world around the vehicle. For the camera modality, good segmentation results are generally a major prerequisite to solving the robotic perception problem.  

\subsection{Relavant Work} 

In the context of urban scenes, the semantic label space is large. Segmentation algorithms must interact with roads, poles, signs, cars, sometimes horse-drawn carriage \cite{b2}. To limit the scope of my problem space, I will focus on cars specifically. While cars are a common occurrence in images, they take up a smaller proportion of samples in the context of binary segmentation. Furthermore, their frequency is governed by other external factors such as time of day, weather conditions, etc. Continuous objects such as roadways and such are far less challenging to detect and segment due to their consistency in most urban environments. If we do not account for this in training our model, our model will learn to produce segmentation masks which produce good general accuracy metrics, but sub-optimal performance on the class of interest. An example of this would be the situation where the network might predict a segmentation mask of "not-car". In this case, the accuracy might be high, but the class-specific accuracy will be poor (ie. failure to predict the object of interest: the pixels associated car). This problem has been a topic of concern in the medical imaging space. For example, in the context of tumor segmentation, tumors can have varying shapes, sizes and locations within a 3D volume. Furthermore, tumors generally take up a smaller portion of the brain than unaffected brain matter. As such, many researchers in this space have attempted to address these issues, by designing loss functions which weight positive examples more than the negative ones. Milletari et al. \cite{b1} propose using a DICE loss which factors in the frequency of certain class observations when computing loss. \\

For segmentation problems in the multi-label space domain, researchers can weigh certain labels depending on their frequency. U-Net \cite{unet} does this by using a Weighted Cross Entropy for their training loss for the multi-class segmentation problem. In reference \cite{unet} -- as well as other networks which make use of class-specific weight hyperparameters  -- the weights are set inversely proportional to their corresponding class frequency in the ground truth data. That means classes with high-frequency will contribute to a smaller value in the loss as compared to their minority-class counterparts. During training, this means that we will adjust our weights "less" in the face of majority-class observations. I will explore some of these methods in the later sections of this paper.




\section{Methods}

\subsection{Architecture} 
For this project, I decided to use the U-Net model. This model was initially used in the context of medical imaging for the use of cell segmentation. The original model features an encoder-decoder pair. The encoder is comprised of a series of "Double Convolution" blocks. Each block consists of 2 3x3 convolutions, followed by a standard 2x2 max-pooling layer. There are 4 of these blocks leading up to the bottleneck. For the decoder portion, there's a series of "Up-Convolution" blocks that correspond to each block in the encoder. These Up-Convolution blocks consist of 2 3x3 Convolutions, followed by an transposed convolution (also known as a fractional convolution) used to up scale the feature maps. The inputs to these "Up-Conv" blocks are a concatenation of 1) the preceding transposed convolution and 2) a center-crop from the corresponding block in the encoder pair. This "cat-crop" mirror is what forms the "U" shape in U-Net. Figure \ref{unet} shows the original architecture of the model. The finally UpConv outputs are followed, by C 1x1 convolutions, where C corresponds to the number of classes. This finally goes into a SoftMax layer for multi-class segmentation. \\   

 
I made a few modifications to help with training and testing. The input image for the standard U-Net was 572x572 and output image was 388x388. The authors \cite{unet} then used morphological operations to resize the segmentation mask to its original space. To speed up training time, I resized my input image to 256x256. Furthermore, I padded my feature maps in the output of all the convolution steps to avoid having to rely on image processing methods to get back to the original size. This simplified and sped up the validation and training processes tremendously. I also strictly use a single 1x1 kernel at the end and a sigmoid activation since I'm simply doing a binary segmentation.  


\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.25]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/unet-model.png}}
\caption{Figure of the original U-Net.}
\label{unet}
\end{figure}


\subsection{Loss Functions}

To address the binary segmentation problem, I experimented with 3 different loss functions: Binary Cross Entropy (BCE), DICE, and a modified version of DICE where I dropped the negative example term. Sudre, C.H., et al. \cite{gen-dice} refer to these losses in depth -- I will be using their variable notation to define the loss functions used in this paper. Let $r_n$ define a ground truth pixel in the semantic map. Let $p_n$ represent a pixel in the predicted semantic map. I define the BCE loss as: \\


\begin{equation}
-\frac{1}{N} \sum_N r_ilog(p_n) + (1 - r_n)log(1 -p_n)
\end{equation}

The DICE loss as:\\


\begin{equation}
1 -\frac{\sum_{i=0}^N p_nr_n + \epsilon}{\sum_{i=0}^N  p_n + r_n + \epsilon} - \frac{\sum_{i=0}^N  (1 - p_n)(1 - r_n)}{\sum_{i=0}^N  2 - p_n - r_n +\epsilon}
\end{equation}

And the "modified" DICE loss as: \\


\begin{equation}
1 -\frac{\sum_{i=0}^N p_nr_n + \epsilon}{\sum_{i=0}^N  p_n + r_n + \epsilon} 
\end{equation}\\

 
The last function was one that I derived. Due to the severe imbalance in the dataset (which will be covered in the Datasets section), I wanted to test the importance of the last term in the DICE loss. Comparatively from a theoretical standpoint, BCE is less suited for imbalanced class problems compared to the overlap-based losses. This comes from the denominator of these overlap-based loss functions. While BCE focuses on raw numbers of positive and negative samples, the DICE-based losses normalize their errors proportional to frequency of the positive-negative observations.    	

\subsection{Datasets}

For this paper, I used two dataset. The first was the KITTI \cite{kitti} dataset. This comprised of 200 images along with semantic maps. The semantic label space consist of 33 different classes. These classes consist of items such as cars, poles, road, etc.I decided to focus strictly on cars segmentation. One difficulty of working with this dataset was that each image had a slightly different size. The dimensions were roughly 1200x375. To normalize the image size, I resized each image and corresponding segmentation mask to a 256x256 patch. Learning and inferencing on images of this size was useful. I was able to achieve good training speed all while maintaining decent performance. I applied a standard 80-10-10 split on the 200 images to generate samples. After converting the multi-class dataset into a binary segmentation one, I computed the proportion of "car" pixels to "non-car" pixels as 0.067. This low value clearly points to a high degree of binary class imbalance. 

\subsection{Training}

For training, I used a standard SGD optimizer with a momentum of 0.9 with an initial learning rate of 0.001. I also added a learning scheduler set to decrease the learning rate by a factor of 10 every time the validation loss stays constant or increases for 10 epochs. I set the model to train for 300 epochs. This was more to bound the computation to a tractable amount, but -- as we'll see in the Experiments section -- the models' converged well within this time. 



\section{Experiments}


\subsection{Training on KITTI Semanitic Segmentation Dataset}

In general, all the models seems to converge reasonably well as seen in Figures \ref{bce-loss}, \ref{dice-mod-loss} and \ref{dice-og-loss}. I noticed that there are some "jagged portions" in the validation loss around the 150-200 epoch range that leads into a plateau. This plateau seemed to occur directly after the loss was lowered by the scheduler. This potentially shows the optimizer continually missing the local minima (ie. underfitting due too large of a learning rate) at the previously higher learning rate.  


\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.45]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/bce-figures/loss.png}}
\caption{Training and Validation Loss for BCE loss on KITTI dataset.}
\label{bce-loss}
\end{figure}


\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.45]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dice-mod-figures/loss.png}}
\caption{Training and Validation Loss for DICE "modified" loss on KITTI dataset.}
\label{dice-mod-loss}
\end{figure}


\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.45]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dice-og-figures/loss.png}}
\caption{Training and Validation Loss for DICE loss on KITTI dataset.}
\label{dice-og-loss}
\end{figure}




\subsection{Evaluation on KITTI Dataset}
 
 

Overall, the model performed well on the KITTI test dataset. Using the AUC measure (See in Figure \ref{pr-curve}) of the PR curve, the ranking favors DICE original, followed by DICE "Modified" and then BCE. This fits my theoretical understanding of learning on unbalanced datasets -- the performance delta between BCE and DICE "mod" is much larger than that of the  DICE "mod" and DICE "original", indicating that the overlap-based approach work better than the raw-frequency-based loss approach. What is interesting is that the DICE "mod" loss does not perform as well as the original DICE loss. This might be because the "negative class" term in the DICE loss acts as a form of regularization. If we only learn features which fit positive examples, then we may over-fit to positive examples in the data.  
\\

Figures \ref{kitti-sample-1-bce}, \ref{kitti-sample-1-dice-mod} and \ref{kitti-sample-1-dice-og} show qualitiative examples of each model on a specific test image where the class-imbalance is particuarily high. In this case, there are 3 cars that are all closely compact wrt. eachother. The most interesting observation to note is that there's a clear false positive in the BCE model around $x \in [0, 10]$ and $y \in [120, 130]$ range that isn't in the other model outputs. This is an example of the BCE-trained model not being able to correctly learn the features associated to the miniority class in the semantic segmentation problem.


\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.45]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/class-imbalance/pr-curve.png}}
\caption{PR Curve Comparison KITTI Test Dataset.}
\label{pr-curve}
\end{figure}



\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.35]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/class-imbalance/test_sample_1-gt.png}}
\caption{Qualitative Sample 1 KITTI Test Dataset. Real sample overlayed with GT binary mask.}
\label{kitti-sample-1-gt}
\end{figure}


\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.35]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/class-imbalance/test_sample_1-bce.png}}
\caption{Qualitative Sample 1 KITTI Test Dataset. BCE-trained model.}
\label{kitti-sample-1-bce}
\end{figure}



\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.35]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/class-imbalance/test_sample_1-dice-mod.png}}
\caption{Qualitative Sample 1 KITTI Test Dataset. DICE "mod"-trained model.}
\label{kitti-sample-1-dice-mod}
\end{figure}



\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.35]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/class-imbalance/test_sample_1-dice-og.png}}
\caption{Qualitative Sample 1 KITTI Test Dataset. DICE "original"-trained model.}
\label{kitti-sample-1-dice-og}
\end{figure}




\subsection{Evaluation on Custom "DC Commute" Dataset}

Unfortunately, the model didn't scale very well to the hand-acquired dataset I put together. This is most likely due to the model being only trained on 200 images with no data augmentation. It would be wise to augment the model by programmatically changing the exposure of the training sample to increase the dataset size. That said, the model results on the test datasets do show some capability of scalable performance. Figures \ref{dc-sample-1-bce} and \ref{dc-sample-1-dice-mod} and \ref{dc-sample-1-dice-og} show the results of the segmentation networks on the test image seen in Figure \ref{dc-sample-1-input}. For these output masks, I did not threshold based on optimal PR result. Instead, I plotted the mask as a heat-map of raw binary probabilities on top of the original image. Here, more red means more "car-like" and blue means more "not-car-like". In all these model outputs', it seems like the models are able to detect the backs of cars towards the left side of the image center (ie. no false positives). But there is clearly a high false negative rate as cars were not accurately detected in this scene. It is possible that the dataset I trained on had a much higher class imbalance that this training example, which would contribute to these poor results.  Another interesting observation to note is that network seems to assign high car probabilities more consistently around the red "reverse" lights. Since the images in the training dataset were taken from an ego-vehicle, a lot of the observations were of backs of cars. The network potentially picked up on the “red-ness” as a "car like feature". Looking at Figure \ref{dc-sample-2-dice-og}, we see that we get a true positive on a red car parked on the side or the road. But we also get a false positive on a red potted plant on the right. This shows that the model has overfit to extremely specific features. The model isn't quite placing attention on the right features for optimal car segmentation.  

\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.50]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dc-commute-test/test_sample_new-input.png}}
\caption{Test sample from "DC Commute" dataset}
\label{dc-sample-1-input}
\end{figure}


\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.50]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dc-commute-test/test_sample_new-bce.png}}
\caption{Qualitative Sample 1 "DC Commute" Test Dataset. BCE-trained model. }
\label{dc-sample-1-bce}
\end{figure}



\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.50]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dc-commute-test/test_sample_new-dice-mod.png}}
\caption{Qualitative Sample 1 "DC Commute" Test Dataset. DICE "mod"-trained model.}
\label{dc-sample-1-dice-mod}
\end{figure}





\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.50]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dc-commute-test/test_sample_new-dice-og.png}}
\caption{Qualitative Sample 1 KITTI Test Dataset. DICE "original"-trained model.}
\label{dc-sample-1-dice-og}
\end{figure}



\begin{figure}[htbp]
\leftline{\includegraphics[scale=0.50]{/home/komelmerchant/Desktop/JHUCourseTracking/DeepLearningForCV/homeworks/jhu_midterm/figures/dc-commute-test/test_sample_new_2-dice-original.png}}
\caption{Qualitative Sample 2 "DC Commute" Test Dataset. DICE original-trained model.}
\label{dc-sample-2-dice-og}
\end{figure}


\section{Discussion}

 
These experiments have shown a few important concepts about segmentation models. 1) Designing a loss function that works with your data is extremely important. If your data is severely imbalanced, choosing an overlap-based loss function is a viable solution to working with this deficiency. 2) It is important to train your model with enough data which captures the different variances of the operating domain. Based on the results of a KITTI-training U-Net on a custom dataset, model training data must capture various aspects to be properly scalable. This includes lighting conditions, type of camera, and various times of day. From an architecture perspective, results on the test dataset show also how important context matters. While U-Net works well on data that exists in the same feature-space distribution, models that explicitly learn the underlying relationship of the different classes may serve better with respect to scalability.  


\section*{Acknowledgment}
The open source KITTI \cite{kitti} dataset proved to be very useful when training my model. Since the dataset is very popular, there are resources and documentation to help interact with the datasets. I would also like to thank Professor Nasser Nasrabadi for answering all of my \textbf{many, many}  e-mail questions on this  in an extremely prompt manner :) 

\begin{thebibliography}{00}
\bibitem{unet} Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. International  Conference  On  Medical  Image  Computing And  Computer-assisted  Intervention.  pp.  234-241 (2015)

\bibitem{b1} Brosch, T., Yoo, Y., Tang, L.Y., Li, D.K., Traboulsee, A., Tam, R.: Deep convolutional encoder networks for multiple sclerosis lesion segmentation. In: MICCAI
2015. pp. 3–11. Springer (2015)

\bibitem{gen-dice} Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Jorge Cardoso, M.: GeneralisedDice overlap as a deep learning loss function for highly unbalanced segmentations.Lecture Notes in Computer Science (including subseries Lecture Notes in ArtificialIntelligence and Lecture Notes in Bioinformatics)10553 LNCS, 240–248 (2017).

\bibitem{kitti} Andreas Geiger, Philip Lenz, and Raquel Urtasun.  Are weready for autonomous driving?   the kitti vision benchmarksuite. In2012 IEEE conference on computer vision and pat-tern recognition, pages 3354–3361. IEEE, 2012.

\bibitem{b2} \url{https://www.reddit.com/r/technology/comments/wt89lv/video_appears_to_show_a_teslas_autopilot_system/}

\bibitem{sensor-fusion} D.  Feng,  A.  Harakeh,  S.  L.  Waslander,  and  K.  Dietmayer,  “A  reviewand comparative study on probabilistic object detection in autonomousdriving,”IEEE  Transactions  on  Intelligent  Transportation  Systems,2021. 

\end{thebibliography}
\vspace{12pt}

\end{document}
